{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTEBOOK 2: END TO END ML USING SNOWPARK AND SCIKIT-LEARN\n",
    "\n",
    "In this notebook we fit/train a Scikit-Learn ML pipeline that includes common feature engineering tasks such as Imputations, Scaling and One-Hot Encoding. The pipeline also includes a `RandomForestRegressor` model that will predict median house values in California. \n",
    "\n",
    "We will fit/train the pipeline using a Snowpark Python Stored Procedure (SPROC) and then save the pipeline to a Snowflake stage. This example concludes by showing how a saved model/pipeline can be loaded and run in a scalable fashion on a snowflake warehouse using Snowpark Python User-Defined Functions (UDFs). \n",
    "\n",
    "![Snowpark ML](/images/snowpark_ml.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a session with Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snowpark\n",
    "import snowflake.snowpark\n",
    "from snowflake.snowpark.functions import sproc\n",
    "from snowflake.snowpark.session import Session\n",
    "from snowflake.snowpark import version as v\n",
    "import json\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import io\n",
    "import joblib\n",
    "\n",
    "with open('creds.json') as f:\n",
    "    data = json.load(f)\n",
    "    USERNAME = data['username']\n",
    "    PASSWORD = data['password']\n",
    "    SF_ACCOUNT = data['sf_account']\n",
    "    SF_WH = data['sf_wh']\n",
    "    SF_DB = data['sf_db']\n",
    "    SF_SCHEMA = data['sf_schema']\n",
    "\n",
    "CONNECTION_PARAMETERS = {\n",
    "   \"account\": SF_ACCOUNT,\n",
    "   \"user\": USERNAME,\n",
    "   \"password\": PASSWORD,\n",
    "   \"database\": SF_DB,\n",
    "   \"schema\": SF_SCHEMA,\n",
    "   \"warehouse\": SF_WH\n",
    "}\n",
    "\n",
    "session = Session.builder.configs(CONNECTION_PARAMETERS).create()\n",
    "session.add_packages('snowflake-snowpark-python', 'scikit-learn', 'pandas', 'numpy', 'joblib', 'cachetools')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create stages to save the ML model/pipeline and permanent UDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"create or replace stage models\" +\\\n",
    "        \" directory = (enable = true)\" +\\\n",
    "        \" copy_options = (on_error='skip_file')\"\n",
    "        \n",
    "session.sql(query).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"create or replace stage udf\" +\\\n",
    "        \" copy_options = (on_error='skip_file')\"\n",
    "        \n",
    "session.sql(query).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stored Proc fits the pipeline and the model and then saves it in Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file(session, model, path):\n",
    "  input_stream = io.BytesIO()\n",
    "  joblib.dump(model, input_stream)\n",
    "  session._conn._cursor.upload_stream(input_stream, path)\n",
    "  return \"successfully created file: \" + path\n",
    "\n",
    "def train_model(session: snowflake.snowpark.Session) -> float:\n",
    "    snowdf = session.table(\"HOUSING_DATA\")\n",
    "    # split the train and test set\n",
    "    snowdf_train, snowdf_test = snowdf.random_split([0.8, 0.2], seed=82) # use seed to make the split repeatable\n",
    "    \n",
    "\n",
    "    # save the train and test sets as time stamped tables in Snowflake \n",
    "    snowdf_train.write.mode(\"overwrite\").save_as_table(\"HOUSING_TRAIN\")\n",
    "    snowdf_test.write.mode(\"overwrite\").save_as_table(\"HOUSING_TEST\")\n",
    "    \n",
    "    housing = snowdf_train.drop(\"MEDIAN_HOUSE_VALUE\").to_pandas() # drop labels for training set\n",
    "    housing_labels = snowdf_train.select(\"MEDIAN_HOUSE_VALUE\").to_pandas()\n",
    "    housing_test = snowdf_test.drop(\"MEDIAN_HOUSE_VALUE\").to_pandas()\n",
    "    housing_test_labels = snowdf_test.select(\"MEDIAN_HOUSE_VALUE\").to_pandas()\n",
    "\n",
    "    # numerical features\n",
    "    housing_num = housing.drop(\"OCEAN_PROXIMITY\", axis=1)\n",
    "    # create a pipeline for numerical features\n",
    "    num_pipeline = Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "            ('std_scaler', StandardScaler()),\n",
    "        ])\n",
    "\n",
    "    num_attribs = list(housing_num)\n",
    "    cat_attribs = [\"OCEAN_PROXIMITY\"]\n",
    "\n",
    "    preprocessor = ColumnTransformer([\n",
    "            (\"num\", num_pipeline, num_attribs),\n",
    "            (\"cat\", OneHotEncoder(), cat_attribs)\n",
    "        ])\n",
    "\n",
    "    full_pipeline = Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('model', RandomForestRegressor(n_estimators=100, random_state=42)),\n",
    "        ])\n",
    "\n",
    "    # fit the preprocessing pipeline and the model together\n",
    "    full_pipeline.fit(housing, housing_labels)\n",
    "\n",
    "    # save the full pipeline including the model\n",
    "    save_file(session, full_pipeline, \"@MODELS/housing_fores_reg.joblib\")\n",
    "\n",
    "    # predict on the test set and return the root mean squared error (RMSE)\n",
    "    housing_predictions = full_pipeline.predict(housing_test)\n",
    "    lin_mse = mean_squared_error(housing_test_labels, housing_predictions)\n",
    "    lin_rmse = np.sqrt(lin_mse)\n",
    "    return lin_rmse\n",
    "\n",
    "# Create an instance of StoredProcedure using the sproc() function\n",
    "train_model_sp = sproc(train_model, replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the training within the SPROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model_sp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model/Pipeline Deployment \n",
    "\n",
    "Define the UDF that loads the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import cachetools\n",
    "import os\n",
    "from snowflake.snowpark.functions import udf\n",
    "session.add_import(\"@MODELS/housing_fores_reg.joblib\")  \n",
    "\n",
    "@cachetools.cached(cache={})\n",
    "def read_file(filename):\n",
    "       import_dir = sys._xoptions.get(\"snowflake_import_directory\")\n",
    "       if import_dir:\n",
    "              with open(os.path.join(import_dir, filename), 'rb') as file:\n",
    "                     m = joblib.load(file)\n",
    "                     return m\n",
    "\n",
    "features = ['LONGITUDE', 'LATITUDE', 'HOUSING_MEDIAN_AGE', 'TOTAL_ROOMS',\n",
    "       'TOTAL_BEDROOMS', 'POPULATION', 'HOUSEHOLDS', 'MEDIAN_INCOME', 'OCEAN_PROXIMITY']\n",
    "\n",
    "@udf(name=\"predict\", is_permanent=True, stage_location=\"@udf\", replace=True)\n",
    "def predict(LONGITUDE: float, LATITUDE: float, HOUSING_MEDIAN_AGE: float, TOTAL_ROOMS: float, \n",
    "                    TOTAL_BEDROOMS: float, POPULATION: float, HOUSEHOLDS: float, MEDIAN_INCOME: float, \n",
    "                    OCEAN_PROXIMITY: str) -> float:\n",
    "       m = read_file('housing_fores_reg.joblib')       \n",
    "       row = pd.DataFrame([locals()], columns=features)\n",
    "       return m.predict(row)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the UDF to make predictions over the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark import functions as F\n",
    "\n",
    "snowdf_test = session.table(\"HOUSING_TEST\")\n",
    "inputs = snowdf_test.drop(\"MEDIAN_HOUSE_VALUE\")\n",
    "snowdf_results = snowdf_test.select(*inputs,\n",
    "                    predict(*inputs).alias('PREDICTION'), \n",
    "                    (F.col('MEDIAN_HOUSE_VALUE')).alias('ACTUAL_LABEL')\n",
    "                    ).limit(20)\n",
    "                    \n",
    "snowdf_results.to_pandas().head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Vectorized UDFs For Optimal Performance\n",
    "\n",
    "The code above runs the model in parallel but performs the predictions row by row. We can further improve it by using vectorized UDFs. Snowpark automatically splits up the rows and sends a batch to each UDF execution resulting in better throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import sys\n",
    "import cachetools\n",
    "import os\n",
    "from snowflake.snowpark.functions import pandas_udf\n",
    "from snowflake.snowpark import types as T\n",
    "\n",
    "features = ['LONGITUDE', 'LATITUDE', 'HOUSING_MEDIAN_AGE', 'TOTAL_ROOMS',\n",
    "       'TOTAL_BEDROOMS', 'POPULATION', 'HOUSEHOLDS', 'MEDIAN_INCOME', 'OCEAN_PROXIMITY']\n",
    "\n",
    "session.add_import(\"@MODELS/housing_fores_reg.joblib\")  \n",
    "@cachetools.cached(cache={})\n",
    "def read_file(filename):\n",
    "       import_dir = sys._xoptions.get(\"snowflake_import_directory\")\n",
    "       if import_dir:\n",
    "              with open(os.path.join(import_dir, filename), 'rb') as file:\n",
    "                     m = joblib.load(file)\n",
    "                     return m\n",
    "\n",
    "@pandas_udf(max_batch_size=100)\n",
    "def predict_batch(df: T.PandasDataFrame[float, float, float, float,\n",
    "                                          float, float, float, float, str]) -> T.PandasSeries[float]:\n",
    "       m = read_file('housing_fores_reg.joblib') \n",
    "       df.columns = features\n",
    "       return m.predict(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now running the vectorized UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark import functions as F\n",
    "\n",
    "snowdf_test = session.table(\"HOUSING_TEST\")\n",
    "inputs = snowdf_test.drop(\"MEDIAN_HOUSE_VALUE\")\n",
    "snowdf_results = snowdf_test.select(*inputs,\n",
    "                    predict_batch(*inputs).alias('PREDICTION'), \n",
    "                    (F.col('MEDIAN_HOUSE_VALUE')).alias('ACTUAL_LABEL')\n",
    "                    ).limit(20)\n",
    "                    \n",
    "snowdf_results.to_pandas().head(20)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7967a0df53185fdb66d4f10f718a455ec94087b64649db4564db11ccde5f15b6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('snowpark_ml_test')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
