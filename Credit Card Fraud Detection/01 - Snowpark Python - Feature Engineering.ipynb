{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6e408fe-988e-49af-986c-52530c763ba9",
   "metadata": {},
   "source": [
    "# Credit Card Fraud Prediction - Feature Engineering\n",
    "\n",
    "This example is based on the Machine Learning for Credit Card Fraud detection - Practical handbook, https://fraud-detection-handbook.github.io/fraud-detection-handbook/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92096f68-5b3b-4c3d-8336-edd0e442b2e3",
   "metadata": {},
   "source": [
    "## Baseline feature transformation\n",
    "\n",
    "### Import Snowpark & Connect to Snowflake\n",
    "\n",
    "Here we first import the Snowpark dependencies and then make a connection with the Snowflake account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f444da-e690-4bd9-9281-d17b212eed14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snowpark\n",
    "from snowflake.snowpark import Session\n",
    "import snowflake.snowpark.types as T\n",
    "import snowflake.snowpark.functions as F\n",
    "from snowflake.snowpark import Window\n",
    "\n",
    "# Print the version of Snowpark we are using\n",
    "from importlib.metadata import version\n",
    "version('snowflake_snowpark_python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af92c74a-bb54-4983-a36a-6a14bd80eb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib as plt\n",
    "plt.rcParams['figure.figsize'] = [18, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6558380b-9a9d-4920-ab2f-f6a9d21f2d4d",
   "metadata": {},
   "source": [
    "**Before connecting make sure you have updated creds.json with information for your Snowflake account**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2045b48-4749-45f6-a61b-140915680d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('creds.json') as f:\n",
    "    connection_parameters = json.load(f)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c003c14-8a9c-4cfa-93f4-96dcf72dd7d5",
   "metadata": {},
   "source": [
    "Connect to Snowflake using the parameters in creds.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f84f71-18b2-4fca-9af0-bdf165f202a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = Session.builder.configs(connection_parameters).create()\n",
    "print(f\"Current schema: {session.get_fully_qualified_current_schema()}, current role: {session.get_current_role()}, current warehouse:  {session.get_current_warehouse()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ba1cc6-ad4a-4d03-8aad-04e6c116b209",
   "metadata": {},
   "source": [
    "### Define a DataFrame\n",
    "\n",
    "We start by defining a Snowpark Dataframe that reference the **CUSTOMER_TRANSACTIONS_FRAUD** table in our database. No data will be pulled back and the **dfCustTrxFraud** is basicly only containing the SQL needed to use the table. The below image gives a illustration on what type of data it has.\n",
    "\n",
    "![Original Data Frame](images/figure1.png)\n",
    "\n",
    "Using the **show** command brings back 10 rows to the client, meaning the SQL to use the table are executed in Snowflake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd624b3b-b179-4a97-b93f-c677c2061b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cust_trx_fraud = session.table(\"CUSTOMER_TRANSACTIONS_FRAUD\")\n",
    "df_cust_trx_fraud.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e981073-3767-43c9-83cd-adac352a8831",
   "metadata": {},
   "source": [
    "## Data Understanding\n",
    "\n",
    "Let's start by getting some basic understanding of our data.\n",
    "\n",
    "We can use the **describe** function on our **numeric** columns to get some basic statistics. The only column in our current dataset would be TX_AMOUNT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb972cb-a458-4d2f-9cf7-1720b498c715",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cust_trx_fraud.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac4c790-7feb-4fdd-9cd5-2f62fdabe706",
   "metadata": {},
   "source": [
    "By aggregating on date and count the number of rows we can visulaize over time, plotly is a good library to do that and inb order to use it we need to pull back our reulst to the client which can be done with **to_pandas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f79aa8-2241-4bd7-a644-809ee371e13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trx_by_day = df_cust_trx_fraud.group_by(F.to_date(F.col(\"TX_DATETIME\"))).count().sort(F.col(\"TO_DATE(TX_DATETIME)\")).select(F.col(\"TO_DATE(TX_DATETIME)\").as_(\"DATE\"), F.col(\"COUNT\")).to_pandas()\n",
    "df_trx_by_day.plot(x=\"DATE\", y=\"COUNT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c491fa3-b853-4aae-92d4-26018251da73",
   "metadata": {},
   "source": [
    "We can see that we have about 6 months  of transactions and around 9k transactions/day.\n",
    "\n",
    "Let's count the number of fraudulent and none fraudulent transactions, by using the **call_builtin** function we can also use the use **RATIO_TO_REPORT** function (that currently is not exposed in the Snowpark API) to also get precentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2670922-f1ae-42ca-8d11-79f9692c13e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cust_trx_fraud.group_by(F.col(\"TX_FRAUD\")).agg(F.count(F.col(\"TRANSACTION_ID\")).as_(\"NB_TX_DAY\"))\\\n",
    "            .select(F.col(\"TX_FRAUD\"), F.col(\"NB_TX_DAY\"), (F.call_builtin(\"RATIO_TO_REPORT\", F.col(\"NB_TX_DAY\")).over() * 100).as_(\"percentage\") )\\\n",
    "            .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116c1711-9b7e-4706-ad67-99db3be4d333",
   "metadata": {},
   "source": [
    "We can see that about 0.9% of all transactions are fraud.\n",
    "\n",
    "If we look at the number of fraudlent transactions and unique cards used we can see that most cards is used very few times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a18d05-1489-40db-a6bf-4e33949f45bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_day_by_card = df_cust_trx_fraud.filter(F.col(\"TX_FRAUD\") == 1)\\\n",
    "                             .group_by(F.to_date(F.col(\"TX_DATETIME\"))).agg([F.sum(F.col(\"TX_FRAUD\")).as_(\"NBR_FRAUD_TRX\"), F.count_distinct(F.col(\"CUSTOMER_ID\")).as_(\"NBR_FRAUD_CARD\")])\\\n",
    "                             .sort(F.col(\"TO_DATE(TX_DATETIME)\")).to_pandas()\n",
    "\n",
    "pd_day_by_card.plot( x=\"TO_DATE(TX_DATETIME)\", y=[\"NBR_FRAUD_TRX\", \"NBR_FRAUD_CARD\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec463eb1-4d74-485b-a9b3-231c02a8d41e",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "### Date and time transformations\n",
    "\n",
    "We will create two new binary features from the transaction dates and times:\n",
    "\n",
    "* The first will characterize whether a transaction occurs during a weekday (value 0) or a weekend (1), and will be called TX_DURING_WEEKEND\n",
    "* The second will characterize whether a transaction occurs during the day (0) or during the night (1). The night is defined as hours that are between 20pm and 6am. It will be called TX_DURING_NIGHT.\n",
    "\n",
    "This can be done using the built in date functions in Snowflake that are exposed in the Snowpark API and will be based on the **TX_DATETIME** column, as illustrated in the image below.\n",
    "\n",
    "![Figure2](images/figure2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197226c3-c02b-4cc5-9e08-74c98d29edef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_date_time_feat = df_cust_trx_fraud.with_columns([\"TX_DURING_WEEKEND\",  \"TX_DURING_NIGHT\"],\\\n",
    "                                            [F.iff((F.dayofweek(F.col(\"TX_DATETIME\")) == F.lit(6)) | (F.dayofweek(F.col(\"TX_DATETIME\")) == F.lit(0)), F.lit(1), F.lit(0)),\\\n",
    "                                             F.iff((F.hour(F.col(\"TX_DATETIME\")) < F.lit(6)) | (F.hour(F.col(\"TX_DATETIME\")) > F.lit(20)), F.lit(1), F.lit(0))])\n",
    "\n",
    "df_date_time_feat.sort(F.col(\"TRANSACTION_ID\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a9befa-10c0-41d3-b66c-bd7d46881fe7",
   "metadata": {},
   "source": [
    "Is there differences between the number of fraud cases during weekdays/weekdays and day/Night?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214a5ec3-fb57-4f74-92a6-4563ff9b514e",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = Window.partition_by(F.col(\"TX_DURING_WEEKEND\"), F.col(\"TX_DURING_NIGHT\"))\n",
    "\n",
    "df_date_time_feat.group_by(F.col(\"TX_DURING_WEEKEND\"),F.col(\"TX_DURING_NIGHT\"),F.col(\"TX_FRAUD\"))\\\n",
    "                    .count().sort(F.col(\"TX_DURING_WEEKEND\"), F.col(\"TX_DURING_NIGHT\"), F.col(\"TX_FRAUD\"))\\\n",
    "                    .select(F.col(\"TX_DURING_WEEKEND\"), F.col(\"TX_DURING_NIGHT\"), F.col(\"TX_FRAUD\"), (F.call_builtin(\"RATIO_TO_REPORT\", F.col(\"COUNT\")).over(window) * 100).as_(\"percentage\") )\\\n",
    "                    .sort(F.col(\"TX_DURING_WEEKEND\"), F.col(\"TX_DURING_NIGHT\"), F.col(\"TX_FRAUD\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7733ffbf-09c2-4c90-8a96-d41539ffe0e9",
   "metadata": {},
   "source": [
    "We can see that basicly the number of fradulent transactions are the same during the days"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234ac069-106b-4749-91eb-9386d1735eed",
   "metadata": {},
   "source": [
    "### Customer spending behaviour transformations\n",
    "\n",
    "We will compute two customer spending behaviour features.\n",
    "\n",
    "The first feature will be the number of transactions that occur within a time window (Frequency). The second will be the average amount spent in these transactions (Monetary value). The time windows will be set to one, seven, and thirty days. \n",
    "\n",
    "The values is to be calculated based on day level where our transactions is on seconds level, the table below show a example of the output for the 1 day window.\n",
    "\n",
    "|CUSTOMER_ID|TX_DATETIME|TX_AMOUNT|AVG 1 DAY WIN|NBR TRX  1 DAY WIN|\n",
    "|:---|:---|:---|:---|:---|\n",
    "|0|2019-04-01 07:19:05.000|123.59|123.59|1|\n",
    "|0|2019-04-01 18:00:16.000|77.34|100.465|2|\n",
    "|0|2019-04-01 19:02:02.000|46.51|82.48|3|\n",
    "|0|2019-04-02 08:51:06.000|54.72|59.523333333|4|\n",
    "|0|2019-04-02 14:05:38.000|63.3|60.4675|5|\n",
    "|0|2019-04-02 15:13:02.000|32.35|54.844|6|\n",
    "|0|2019-04-02 15:46:51.000|13.59|47.968333333|7|\n",
    "|0|2019-04-02 20:24:47.000|51.89|43.17|8|\n",
    "|0|2019-04-03 07:41:24.000|93.26|51.518333333|6|\n",
    "\n",
    "Since we want to aggregate by day and also take include dates that has no transactions (so our windows are real days) we need to first create a new data frame that has for each customer one row for each date between the minimum transaction date and maximum transaction date. Snowpark has a function, **range** , that can be used to generate n number of rows. Since we want to generate a row for each date between minimum and maximum we need to calculate that first. Once we have that dataframe we can do a cross join with our **CUSTOMER** table to create a new dataframe that has now one row for each date between the minimum transaction date and maximum transaction date and customer, as illustrated in the image below.\n",
    "\n",
    "![Figure3](images/figure3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c0c170-174e-4bec-8f66-e28e4af53298",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "date_info = df_cust_trx_fraud.select(F.min(F.col(\"TX_DATETIME\")).as_(\"END_DATE\"), F.datediff(\"DAY\", F.col(\"END_DATE\"), F.max(F.col(\"TX_DATETIME\"))).as_(\"NO_DAYS\")).to_pandas()\n",
    "days = int(date_info['NO_DAYS'].values[0])\n",
    "start_date = str(date_info['END_DATE'].values[0].astype('datetime64[D]'))\n",
    "\n",
    "# Create a dataframe with one row for each date between the min and max transaction date\n",
    "df_days = session.range(days).with_column(\"TX_DATE\", F.to_date(F.dateadd(\"DAY\", F.call_builtin(\"SEQ4\"), F.lit(start_date))))\n",
    "\n",
    "# Since we aggregate by customer and day and not all customers have transactions for all dates we cross join our date dataframe with our \n",
    "# customer table so each customer witll have one row for each date\n",
    "df_customers = session.table(\"CUSTOMERS\").select(\"CUSTOMER_ID\")\n",
    "df_cust_day = df_days.join(df_customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b451c583-c9ca-4822-a9a7-b608cf2fc439",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cust_day.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d83b16-a8fc-446d-83b9-69803510f228",
   "metadata": {},
   "source": [
    "We can now use the new data frame, **df_cust_day**, to aggregate the number of transaction and transaction amount by day, for days that a customer has no transaction we will use 0. The picture below illustrates what we are doing.\n",
    "\n",
    "Earlier in the data understanding part we used **call_builtin** to use a Snowflake functions that is not exposed in the SNowpark API, we can also use **builtin** to assign the function to a variable that we then can use instead how having to write call_builtin each time.\n",
    "\n",
    "\n",
    "\n",
    "![Figure4](images/figure4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474fabdc-6b7d-470d-b9e6-bd356c24e66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_if_null = F.builtin(\"ZEROIFNULL\") # Snowpark does not expose ZEROIFNULL but we can use the call_builtin function for that or the builtin to add it to a variable that can be used in our code\n",
    "\n",
    "df_cust_trx_day = df_cust_trx_fraud.join(df_cust_day, (df_cust_trx_fraud.col(\"CUSTOMER_ID\") == df_cust_day.col(\"CUSTOMER_ID\")) & (F.to_date(df_cust_trx_fraud.col(\"TX_DATETIME\")) == df_cust_day.col(\"TX_DATE\")), \"rightouter\") \\\n",
    "            .select(df_cust_day.col(\"CUSTOMER_ID\").as_(\"CUSTOMER_ID\"),\\\n",
    "                    df_cust_day.col(\"TX_DATE\"),\\\n",
    "                    zero_if_null(df_cust_trx_fraud.col(\"TX_AMOUNT\")).as_(\"TX_AMOUNT\"),\\\n",
    "                    F.iff(F.col(\"TX_AMOUNT\") > F.lit(0), F.lit(1), F.lit(0)).as_(\"NO_TRX\"))\\\n",
    "                .group_by(F.col(\"CUSTOMER_ID\"), F.col(\"TX_DATE\"))\\\n",
    "                .agg([F.sum(F.col(\"TX_AMOUNT\")).as_(\"TOT_AMOUNT\"), F.sum(F.col(\"NO_TRX\")).as_(\"NO_TRX\")])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6673cc31-a29c-4cde-8282-8ba3f468b576",
   "metadata": {},
   "source": [
    "Now when we have the number of transactions and amount by customer and day we can aggregate by our windows (1, 7 and 30 days).\n",
    "\n",
    "For getting values previous day we will use the **lag** function since it's only one value we want and for 7 and 30 days we will use sum over a window. Since we do not want to include future values we will not include the current day in the windows.\n",
    "\n",
    "![Figure5](images/figure5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72266105-e1f2-44a4-bb4e-bde55331e576",
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_date = Window.partition_by(F.col(\"customer_id\")).orderBy(F.col(\"TX_DATE\"))\n",
    "win_7d_cust = cust_date.rowsBetween(-7, -1)\n",
    "win_30d_cust = cust_date.rowsBetween(-30, -1)\n",
    "\n",
    "df_cust_feat_day = df_cust_trx_day.select(F.col(\"TX_DATE\"),F.col(\"CUSTOMER_ID\"),F.col(\"NO_TRX\"),F.col(\"TOT_AMOUNT\"),\n",
    "                              F.lag(F.col(\"NO_TRX\"),1).over(cust_date).as_(\"CUST_TX_PREV_1\"),\n",
    "                              F.sum(F.col(\"NO_TRX\")).over(win_7d_cust).as_(\"CUST_TX_PREV_7\"),\n",
    "                              F.sum(F.col(\"NO_TRX\")).over(win_30d_cust).as_(\"CUST_TX_PREV_30\"),\n",
    "                              F.lag(F.col(\"TOT_AMOUNT\"),1).over(cust_date).as_(\"CUST_TOT_AMT_PREV_1\"),\n",
    "                              F.sum(F.col(\"TOT_AMOUNT\")).over(win_7d_cust).as_(\"CUST_TOT_AMT_PREV_7\"),\n",
    "                              F.sum(F.col(\"TOT_AMOUNT\")).over(win_30d_cust).as_(\"CUST_TOT_AMT_PREV_30\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673822bc-e2a6-4d6e-a1c6-898803e4809e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cust_feat_day.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ae0ace-e210-425d-a072-460dfc8b1491",
   "metadata": {},
   "source": [
    "Now we know for each customer and day the number of transactions and amount for previous 1, 7 and 30 days and we add that to our transactions.\n",
    "\n",
    "In this step we will also use a window function to count the number of transactions and total amount for the current date, in order to only include the previous transactions for the same date we will create a partion key that consists of transaction date and customer id. By using that we get a rolling sum of all previous rows that is for the same date and customer.\n",
    "\n",
    "![Figure6](images/figure6.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238d825a-6d81-43ca-9d45-00bb310306cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "win_cur_date = Window.partition_by(F.col(\"PARTITION_KEY\")).order_by(F.col(\"TX_DATETIME\")).rangeBetween(Window.unboundedPreceding,Window.currentRow)\n",
    "\n",
    "df_cust_behaviur_feat = df_date_time_feat.join(df_cust_feat_day, (df_date_time_feat.col(\"CUSTOMER_ID\") == df_cust_feat_day.col(\"CUSTOMER_ID\")) & \\\n",
    "                                             (F.to_date(df_date_time_feat.col(\"TX_DATETIME\")) == df_cust_feat_day.col(\"TX_DATE\")))\\\n",
    "        .with_column(\"PARTITION_KEY\", F.concat(df_date_time_feat.col(\"CUSTOMER_ID\"), F.to_date(df_date_time_feat.col(\"TX_DATETIME\"))))\\\n",
    "        .with_columns([\"CUR_DAY_TRX\",\\\n",
    "                         \"CUR_DAY_AMT\"],\\\n",
    "                      [F.count(df_date_time_feat.col(\"CUSTOMER_ID\")).over(win_cur_date), \\\n",
    "                      F.sum(df_date_time_feat.col(\"TX_AMOUNT\")).over(win_cur_date)])\\\n",
    "        .select(df_date_time_feat.col(\"TRANSACTION_ID\"), \\\n",
    "                df_date_time_feat.col(\"CUSTOMER_ID\").as_(\"CUSTOMER_ID\"), \\\n",
    "                df_date_time_feat.col(\"TERMINAL_ID\"),\\\n",
    "                df_date_time_feat.col(\"TX_DATETIME\").as_(\"TX_DATETIME\"), \\\n",
    "                df_date_time_feat.col(\"TX_AMOUNT\"),\\\n",
    "                df_date_time_feat.col(\"TX_TIME_SECONDS\"),\\\n",
    "                df_date_time_feat.col(\"TX_TIME_DAYS\"), \\\n",
    "                df_date_time_feat.col(\"TX_FRAUD\"),\\\n",
    "                df_date_time_feat.col(\"TX_FRAUD_SCENARIO\"),\\\n",
    "                df_date_time_feat.col(\"TX_DURING_WEEKEND\"), \\\n",
    "                df_date_time_feat.col(\"TX_DURING_NIGHT\"),\\\n",
    "                (zero_if_null(df_cust_feat_day.col(\"CUST_TX_PREV_1\")) + F.col(\"CUR_DAY_TRX\")).as_(\"CUST_CNT_TX_1\"),\\\n",
    "                ((zero_if_null(df_cust_feat_day.col(\"CUST_TOT_AMT_PREV_1\")) + F.col(\"CUR_DAY_AMT\")) / F.col(\"CUST_CNT_TX_1\")).as_(\"CUST_AVG_AMOUNT_1\"), \\\n",
    "                (zero_if_null(df_cust_feat_day.col(\"CUST_TX_PREV_7\")) + F.col(\"CUR_DAY_TRX\")).as_(\"CUST_CNT_TX_7\"),\\\n",
    "                ((zero_if_null(df_cust_feat_day.col(\"CUST_TOT_AMT_PREV_7\")) + F.col(\"CUR_DAY_AMT\")) / F.col(\"CUST_CNT_TX_7\")).as_(\"CUST_AVG_AMOUNT_7\"),\\\n",
    "                (zero_if_null(df_cust_feat_day.col(\"CUST_TX_PREV_30\")) + F.col(\"CUR_DAY_TRX\")).as_(\"CUST_CNT_TX_30\"),\\\n",
    "                ((zero_if_null(df_cust_feat_day.col(\"CUST_TOT_AMT_PREV_30\")) + F.col(\"CUR_DAY_AMT\")) / F.col(\"CUST_CNT_TX_30\")).as_(\"CUST_AVG_AMOUNT_30\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993b4bc3-eb6f-4c82-a5a0-c8d5693b7f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cust_behaviur_feat.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104cbf4d-1ff8-4526-a91f-4cbe0bfea43b",
   "metadata": {},
   "source": [
    "### Terminal ID transformations\n",
    "\n",
    "The main goal with the Terminal ID transformations will be to extract a risk score, that assesses the exposure of a given terminal ID to fraudulent transactions. The risk score will be defined as the average number of fraudulent transactions that occurred on a terminal ID over a time window. As for customer ID transformations, we will use three window sizes, of 1, 7, and 30 days.\n",
    "\n",
    "Contrary to customer ID transformations, the time windows will not directly precede a given transaction. Instead, they will be shifted back by a delay period. The delay period accounts for the fact that, in practice, the fraudulent transactions are only discovered after a fraud investigation or a customer complaint. Hence, the fraudulent labels, which are needed to compute the risk score, are only available after this delay period. To a first approximation, this delay period will be set to one week.\n",
    "\n",
    "Part from above the logic is rather similar to how we calculated for customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec94fc44-a0de-4957-92e7-2ee1162f7b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we aggregate by terminal and day and not all terminals have transactions for all dates we cross join our date dataframe with our terminal table so each terminal will have one row for each date\n",
    "df_terminals = session.table(\"TERMINALS\").select(\"TERMINAL_ID\")\n",
    "df_term_day = df_days.join(df_terminals)\n",
    "\n",
    "# Aggregate number of transactions and amount by terminal and date, for dates where a terminal do not have any ttransactions we ad a 0\n",
    "df_term_trx_by_day = df_cust_trx_fraud.join(df_term_day, (df_cust_trx_fraud.col(\"TERMINAL_ID\") == df_term_day.col(\"TERMINAL_ID\"))\\\n",
    "                    & (F.to_date(df_cust_trx_fraud.col(\"TX_DATETIME\")) == df_term_day.col(\"TX_DATE\")), \"rightouter\")\\\n",
    "                .select(df_term_day.col(\"TERMINAL_ID\").as_(\"TERMINAL_ID\"),\\\n",
    "                        df_term_day.col(\"TX_DATE\"), \\\n",
    "                        zero_if_null(df_cust_trx_fraud.col(\"TX_FRAUD\")).as_(\"NB_FRAUD\"), \\\n",
    "                        F.when(F.is_null(df_cust_trx_fraud.col(\"TX_FRAUD\")), F.lit(0)).otherwise(F.lit(1)).as_(\"NO_TRX\")) \\\n",
    "                .groupBy(F.col(\"TERMINAL_ID\"), F.col(\"TX_DATE\"))\\\n",
    "                .agg([F.sum(F.col(\"NB_FRAUD\")).as_(\"NB_FRAUD\"), F.sum(F.col(\"NO_TRX\")).as_(\"NO_TRX\")])\n",
    "\n",
    "# Aggregate by our windows.\n",
    "term_date = Window.partitionBy(F.col(\"TERMINAL_ID\")).orderBy(F.col(\"TX_DATE\"))\n",
    "win_delay = term_date.rowsBetween(-7, -1) \n",
    "win_1d_term = term_date.rowsBetween(-8, -1) # We need to add the Delay period to our windows\n",
    "win_7d_term = term_date.rowsBetween(-14, -1)\n",
    "win_30d_term = term_date.rowsBetween(-37, -1)\n",
    "\n",
    "df_term_feat_day = df_term_trx_by_day.select(F.col(\"TX_DATE\"),F.col(\"TERMINAL_ID\"),F.col(\"NO_TRX\"), F.col(\"NB_FRAUD\"),\n",
    "                              F.sum(F.col(\"NB_FRAUD\")).over(win_delay).as_(\"NB_FRAUD_DELAY\"),\n",
    "                              F.sum(F.col(\"NO_TRX\")).over(win_delay).as_(\"NB_TX_DELAY\"),\n",
    "                              F.sum(F.col(\"NO_TRX\")).over(win_1d_term).as_(\"NB_TX_DELAY_WINDOW_1\"),\n",
    "                              F.sum(F.col(\"NO_TRX\")).over(win_1d_term).as_(\"NB_TX_DELAY_WINDOW_7\"),\n",
    "                              F.sum(F.col(\"NO_TRX\")).over(win_30d_term).as_(\"NB_TX_DELAY_WINDOW_30\"),\n",
    "                              F.sum(F.col(\"NB_FRAUD\")).over(win_1d_term).as_(\"NB_FRAUD_DELAY_WINDOW_1\"),\n",
    "                              F.sum(F.col(\"NB_FRAUD\")).over(win_1d_term).as_(\"NB_FRAUD_DELAY_WINDOW_7\"),\n",
    "                              F.sum(F.col(\"NB_FRAUD\")).over(win_30d_term).as_(\"NB_FRAUD_DELAY_WINDOW_30\"))\n",
    "\n",
    "df_term_behaviur_feat = df_cust_behaviur_feat.join(df_term_feat_day, (df_cust_behaviur_feat.col(\"TERMINAL_ID\") == df_term_feat_day.col(\"TERMINAL_ID\")) &\\\n",
    "                                                 (F.to_date(df_cust_behaviur_feat.col(\"TX_DATETIME\")) == df_term_feat_day.col(\"TX_DATE\")))\\\n",
    "            .with_columns([\"PARTITION_KEY\",\\\n",
    "                             \"CUR_DAY_TRX\",\\\n",
    "                             \"CUR_DAY_FRAUD\"],\\\n",
    "                         [F.concat(df_cust_behaviur_feat.col(\"TERMINAL_ID\"), F.to_date(df_cust_behaviur_feat.col(\"TX_DATETIME\"))),\\\n",
    "                             F.count(df_cust_behaviur_feat.col(\"TERMINAL_ID\")).over(win_cur_date),\\\n",
    "                             F.sum(df_cust_behaviur_feat.col(\"TX_FRAUD\")).over(win_cur_date)]\\\n",
    "                          )\\\n",
    "             .with_columns([\"NB_TX_DELAY\", \\\n",
    "                              \"NB_FRAUD_DELAY\",\\\n",
    "                              \"NB_TX_DELAY_WINDOW_1\",\\\n",
    "                              \"NB_FRAUD_DELAY_WINDOW_1\",\\\n",
    "                              \"NB_TX_DELAY_WINDOW_7\",\\\n",
    "                              \"NB_FRAUD_DELAY_WINDOW_7\",\\\n",
    "                              \"NB_TX_DELAY_WINDOW_30\",\\\n",
    "                              \"NB_FRAUD_DELAY_WINDOW_30\"],\\\n",
    "                           [df_term_feat_day.col(\"NB_TX_DELAY\") + F.col(\"CUR_DAY_TRX\"),\\\n",
    "                               F.col(\"NB_FRAUD_DELAY\") +  F.col(\"CUR_DAY_FRAUD\"),\\\n",
    "                               F.col(\"NB_TX_DELAY_WINDOW_1\") + F.col(\"CUR_DAY_TRX\"),\\\n",
    "                               F.col(\"NB_FRAUD_DELAY_WINDOW_1\") + F.col(\"CUR_DAY_FRAUD\"),\\\n",
    "                               F.col(\"NB_TX_DELAY_WINDOW_7\") + F.col(\"CUR_DAY_TRX\"),\\\n",
    "                               F.col(\"NB_FRAUD_DELAY_WINDOW_7\") + F.col(\"CUR_DAY_FRAUD\"),\\\n",
    "                               F.col(\"NB_TX_DELAY_WINDOW_30\") + F.col(\"CUR_DAY_TRX\"),\\\n",
    "                               F.col(\"NB_FRAUD_DELAY_WINDOW_30\") + F.col(\"CUR_DAY_FRAUD\")])\\\n",
    "             .select(df_cust_behaviur_feat.col(\"TRANSACTION_ID\"), \\\n",
    "                     df_cust_behaviur_feat.col(\"TX_DATETIME\").as_(\"TX_DATETIME\"),\\\n",
    "                     df_cust_behaviur_feat.col(\"CUSTOMER_ID\").as_(\"CUSTOMER_ID\"), \\\n",
    "                     df_cust_behaviur_feat.col(\"TERMINAL_ID\").as_(\"TERMINAL_ID\"),\\\n",
    "                     df_cust_behaviur_feat.col(\"TX_TIME_SECONDS\"), \\\n",
    "                     df_cust_behaviur_feat.col(\"TX_TIME_DAYS\"), \\\n",
    "                     df_cust_behaviur_feat.col(\"TX_AMOUNT\"), \\\n",
    "                     df_cust_behaviur_feat.col(\"TX_FRAUD\"), \\\n",
    "                     df_cust_behaviur_feat.col(\"TX_FRAUD_SCENARIO\"),\\\n",
    "                     df_cust_behaviur_feat.col(\"TX_DURING_WEEKEND\"), \\\n",
    "                     df_cust_behaviur_feat.col(\"TX_DURING_NIGHT\"),\\\n",
    "                     df_cust_behaviur_feat.col(\"CUST_AVG_AMOUNT_1\"),\\\n",
    "                     df_cust_behaviur_feat.col(\"CUST_CNT_TX_1\"), \\\n",
    "                     df_cust_behaviur_feat.col(\"CUST_AVG_AMOUNT_7\"),\\\n",
    "                     df_cust_behaviur_feat.col(\"CUST_CNT_TX_7\"), \\\n",
    "                     df_cust_behaviur_feat.col(\"CUST_AVG_AMOUNT_30\"),\\\n",
    "                     df_cust_behaviur_feat.col(\"CUST_CNT_TX_30\"),\\\n",
    "                     (F.col(\"NB_TX_DELAY_WINDOW_1\") - F.col(\"NB_TX_DELAY\")).as_(\"NB_TX_WINDOW_1\"),\\\n",
    "                     F.iff(F.col(\"NB_FRAUD_DELAY_WINDOW_1\") - F.col(\"NB_FRAUD_DELAY\") > 0, \\\n",
    "                             (F.col(\"NB_FRAUD_DELAY_WINDOW_1\") - F.col(\"NB_FRAUD_DELAY\")) / F.col(\"NB_TX_WINDOW_1\"), F.lit(0)).as_(\"TERM_RISK_1\"),\\\n",
    "                     (F.col(\"NB_TX_DELAY_WINDOW_7\") - F.col(\"NB_TX_DELAY\")).as_(\"NB_TX_WINDOW_7\"),\\\n",
    "                     F.iff(F.col(\"NB_FRAUD_DELAY_WINDOW_7\") - F.col(\"NB_FRAUD_DELAY\") > 0, \\\n",
    "                             (F.col(\"NB_FRAUD_DELAY_WINDOW_7\") - F.col(\"NB_FRAUD_DELAY\")) / F.col(\"NB_TX_WINDOW_7\"), F.lit(0)).as_(\"TERM_RISK_7\"),\\\n",
    "                     (F.col(\"NB_TX_DELAY_WINDOW_30\") - F.col(\"NB_TX_DELAY\")).as_(\"NB_TX_WINDOW_30\"),\\\n",
    "                     F.iff(F.col(\"NB_FRAUD_DELAY_WINDOW_30\") - F.col(\"NB_FRAUD_DELAY\") > 0, \\\n",
    "                             (F.col(\"NB_FRAUD_DELAY_WINDOW_30\") - F.col(\"NB_FRAUD_DELAY\"))  / F.col(\"NB_TX_WINDOW_30\"), F.lit(0)).as_(\"TERM_RISK_30\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a589de6-6b3e-45f0-96a1-372c6be4e54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_term_behaviur_feat.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1528a7-dab8-4d87-9288-d2b52fe9804a",
   "metadata": {},
   "source": [
    "We now have our new features and can save it into a new table that we then can use for traing our model for predicting fraud.\n",
    "\n",
    "Start by checking the schema of our new dataframe and decide if we will keep all columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62abece7-8aa4-45f7-92b5-9c7ee514d5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_term_behaviur_feat.schema.fields:\n",
    "    print(f\"{col.name}, Nullable: {col.nullable}, {col.datatype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4d1c39-5e57-4597-b858-11c74a168c28",
   "metadata": {},
   "source": [
    "As mentioned we are not actually executing anything in Snowflake, unless when using .show(), and has just created a execution plan.\n",
    "\n",
    "We can get the SQL needed to prefrom all our steps by calling **queries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65054274-99f9-421d-9ba6-6f15b50108d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_term_behaviur_feat.queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18669d28-08ec-419e-98a5-be24038ccae5",
   "metadata": {},
   "source": [
    "We can quicly scale up our Warhouse (compute) before we load our new table and after we done the load we can scale it down again.\n",
    "\n",
    "By creating a function for it we can simplify the code and make it easier for the developers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d051b5-0829-472c-8492-a23eb4533f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_wh(sess, wh, size):\n",
    "    if (len(wh) == 0): \n",
    "        return false\n",
    "    if (len(size) == 0):\n",
    "        return false\n",
    "   \n",
    "    alter_SQL = \"ALTER WAREHOUSE \" + wh + \" SET WAREHOUSE_SIZE = \" + size\n",
    "    sess.sql(alter_SQL).collect()\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db21b879-c099-4a17-8836-26a2e5f98fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_wh(session, session.get_current_warehouse(), \"LARGE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1c007f-0623-4505-891d-d645ba93f12c",
   "metadata": {},
   "source": [
    "We we run **saveAsTable** Snowpark will generate the SQL for all the previous step, execute it in Snowflake and store the result in the table **customer_trx_fraud_features**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f3aa46-4bdf-4f4f-9897-16a1644be874",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_term_behaviur_feat.write.mode(\"overwrite\").save_as_table(\"customer_trx_fraud_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d1ee06-f6a4-4f4a-aa53-8507fb658642",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_wh(session,session.get_current_warehouse(), \"SMALL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3041a5-ea38-40c0-85d1-30f86aa608a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
